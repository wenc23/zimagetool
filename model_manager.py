"""
æ¨¡å‹ç®¡ç†å™¨æ¨¡å—
ç»Ÿä¸€ç®¡ç†æ¨¡å‹åŠ è½½ã€ä¼˜åŒ–æ¨¡å¼åº”ç”¨å’ŒçŠ¶æ€ç®¡ç†
"""

import torch
import time
import threading
from pathlib import Path
from typing import Optional, Tuple
from diffusers import ZImagePipeline


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ç±» - å•ä¾‹æ¨¡å¼ç®¡ç†æ¨¡å‹å®ä¾‹"""

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        """å®ç°å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨"""
        if not hasattr(self, '_initialized') or not self._initialized:
            self.pipe = None
            self.model_loaded = False
            self.loading_in_progress = False
            self.optimization_mode = None
            self._initialized = True

    def load_model(self, optimization_mode: str = "basic", model_path: Optional[str] = None) -> Tuple[bool, str]:
        """
        åŠ è½½æ¨¡å‹

        Args:
            optimization_mode: ä¼˜åŒ–æ¨¡å¼ ("basic" æˆ– "low_vram")
            model_path: æ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä¸º "models/Z-Image-Turbo"

        Returns:
            (æˆåŠŸæ ‡å¿—, æ¶ˆæ¯)
        """
        # å¦‚æœæ¨¡å‹å·²ç»åŠ è½½ï¼Œç›´æ¥è¿”å›
        if self.model_loaded and self.pipe is not None:
            return True, "âœ… æ¨¡å‹å·²åŠ è½½ï¼Œæ— éœ€é‡å¤åŠ è½½"

        # å¦‚æœæ­£åœ¨åŠ è½½ä¸­ï¼Œç­‰å¾…
        if self.loading_in_progress:
            return False, "ğŸ”„ æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨å€™..."

        # è®¾ç½®æ¨¡å‹è·¯å¾„
        local_model_path = Path(model_path or "models/Z-Image-Turbo")

        if not local_model_path.exists():
            return False, f"âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {local_model_path}"

        try:
            self.loading_in_progress = True
            start_time = time.time()

            if optimization_mode == "low_vram":
                # ä½æ˜¾å­˜ä¼˜åŒ–æ¨¡å¼
                self.pipe = ZImagePipeline.from_pretrained(
                    str(local_model_path),
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,
                    local_files_only=True,
                    offload_folder="offload",
                )

                # åº”ç”¨ä½æ˜¾å­˜ä¼˜åŒ–
                self._apply_low_vram_optimizations()
            else:
                # åŸºç¡€ä¼˜åŒ–æ¨¡å¼
                self.pipe = ZImagePipeline.from_pretrained(
                    str(local_model_path),
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,
                    local_files_only=True,
                    device_map="balanced",
                )

                # å¯ç”¨åŸºæœ¬æ˜¾å­˜ä¼˜åŒ–
                self.pipe.enable_attention_slicing("max")

            load_time = time.time() - start_time
            self.model_loaded = True
            self.optimization_mode = optimization_mode
            self.loading_in_progress = False

            return True, f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! è€—æ—¶: {load_time:.2f}ç§’"

        except Exception as e:
            self.loading_in_progress = False
            return False, f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}"

    def _apply_low_vram_optimizations(self):
        """åº”ç”¨ä½æ˜¾å­˜ä¼˜åŒ–æ–¹æ³•"""
        if not self.pipe:
            return

        print("ğŸ”§ å¯ç”¨ä½æ˜¾å­˜ä¼˜åŒ–æ¨¡å¼...")

        try:
            # å…ˆé‡ç½®è®¾å¤‡æ˜ å°„ï¼Œä»¥ä¾¿å¯ç”¨CPUå¸è½½
            if hasattr(self.pipe, 'reset_device_map'):
                print("ğŸ”„ é‡ç½®è®¾å¤‡æ˜ å°„...")
                self.pipe.reset_device_map()

            # å¯ç”¨æ‰€æœ‰å¯ç”¨çš„å†…å­˜ä¼˜åŒ–æŠ€æœ¯
            self.pipe.enable_attention_slicing("max")  # æœ€å¤§åˆ‡ç‰‡
            self.pipe.enable_sequential_cpu_offload()  # é¡ºåºCPUå¸è½½

            print("âœ… ä½æ˜¾å­˜ä¼˜åŒ–å·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ å¯ç”¨ä½æ˜¾å­˜ä¼˜åŒ–æ—¶å‡ºé”™: {e}")
            print("ğŸ’¡ å°è¯•ä½¿ç”¨åŸºæœ¬ä¼˜åŒ–...")
            # å¦‚æœå‡ºé”™ï¼Œè‡³å°‘å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡
            self.pipe.enable_attention_slicing("max")
            print("âœ… å·²å¯ç”¨åŸºæœ¬ä¼˜åŒ–")

    def get_pipe(self):
        """è·å–æ¨¡å‹ç®¡é“å®ä¾‹"""
        return self.pipe

    def is_model_loaded(self):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self.model_loaded and self.pipe is not None

    def get_optimization_mode(self):
        """è·å–å½“å‰ä¼˜åŒ–æ¨¡å¼"""
        return self.optimization_mode

    def reset(self):
        """é‡ç½®æ¨¡å‹ç®¡ç†å™¨çŠ¶æ€"""
        self.pipe = None
        self.model_loaded = False
        self.loading_in_progress = False
        self.optimization_mode = None


# åˆ›å»ºå…¨å±€å®ä¾‹
model_manager = ModelManager()


def load_model(optimization_mode: str = "basic", model_path: Optional[str] = None) -> Tuple[bool, str]:
    """ä¾¿æ·å‡½æ•°ï¼šåŠ è½½æ¨¡å‹"""
    return model_manager.load_model(optimization_mode, model_path)


def get_pipe():
    """ä¾¿æ·å‡½æ•°ï¼šè·å–æ¨¡å‹ç®¡é“"""
    return model_manager.get_pipe()


def is_model_loaded():
    """ä¾¿æ·å‡½æ•°ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
    return model_manager.is_model_loaded()


def get_optimization_mode():
    """ä¾¿æ·å‡½æ•°ï¼šè·å–ä¼˜åŒ–æ¨¡å¼"""
    return model_manager.get_optimization_mode()